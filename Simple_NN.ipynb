{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compatible-administrator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "from collections import defaultdict\n",
    "from optim.optimizer import CosineWarmupScheduler\n",
    "from run.train import training\n",
    "from run.test import test\n",
    "from models.model_config import AuxiliaryTransformer, Transformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.cm import coolwarm\n",
    "plt.set_cmap('cividis')\n",
    "\n",
    "\n",
    "from scipy import spatial\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import itertools\n",
    "from itertools import tee\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "herbal-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 10)  #\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 6)\n",
    "        self.fc4 = nn.Linear(6, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Applying relu non-linearity\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "upper-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inp_data, out_data):\n",
    "    #t_start\n",
    "    inp_data[:,0] /= 600    # 10 mins\n",
    "    #t_operating\n",
    "    #inp_data[:,1] /= (86400*5)    # 1 day\n",
    "    inp_data[:,1] = np.log(inp_data[:,1]) \n",
    "    #t_tracktime\n",
    "    #inp_data[:,2] /= 2592000   # 1 month\n",
    "    inp_data[:,2] = np.log(inp_data[:,2]) \n",
    "    # power \n",
    "    inp_data[:,3] /= 500\n",
    "    #guidevane\n",
    "    inp_data[:,4] /= 180\n",
    "    # draft_pressure\n",
    "    inp_data[:,5] /= 500\n",
    "    # spiral_pressure\n",
    "    inp_data[:,6] /= 10000\n",
    "    # rotational speed\n",
    "    inp_data[:,7] /= 500\n",
    "    \n",
    "    ## output\n",
    "    out_data /= 2000\n",
    "    \n",
    "    return inp_data, out_data\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prescription-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1350000, 11) (1350000, 6)\n",
      "[1.49223927e+02 1.16750037e+05 8.51013559e+05 2.98626510e+02\n",
      " 9.12187176e+01 1.55861493e+02 5.31218745e+03 1.07874637e+02\n",
      " 9.97639259e-01]\n",
      "[1608.72025976 1487.72319511 1688.32415064 1602.36688526 1636.97079258\n",
      " 1678.00840345]\n",
      "0 0\n",
      "[0.24870655       -inf       -inf 0.59725302 0.50677065 0.31172299\n",
      " 0.53121874 0.21574927 0.99763926]\n",
      "[0.80436013 0.7438616  0.84416208 0.80118344 0.8184854  0.8390042 ]\n",
      "Max : [ 0.40833333 12.89064819 14.43935053  0.64660466  0.53433622  0.53671198\n",
      "  0.55127715  0.2167375   1.        ]\n",
      "Max : [0.81010815 0.74734008 0.84716078 0.80274691 0.81949943 0.84172924]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "0.4083333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4e8fba96dbee>:6: RuntimeWarning: divide by zero encountered in log\n",
      "  inp_data[:,1] = np.log(inp_data[:,1])\n",
      "<ipython-input-3-4e8fba96dbee>:9: RuntimeWarning: divide by zero encountered in log\n",
      "  inp_data[:,2] = np.log(inp_data[:,2])\n"
     ]
    }
   ],
   "source": [
    "f_dataset = np.load('Input_to_NN_notNAN.npz', allow_pickle = True)\n",
    "print(f_dataset['X'].shape, f_dataset['Y'].shape )\n",
    "\n",
    "inp_data = f_dataset['X'][:, [0,1,2,4,6,7,8,9,10] ]\n",
    "out_data = f_dataset['Y'] \n",
    "\n",
    "print(np.mean(inp_data, axis =0))\n",
    "print(np.mean(out_data, axis =0))\n",
    "\n",
    "print(np.count_nonzero(np.isnan(f_dataset['X'])), np.count_nonzero(np.isnan(f_dataset['Y'])))\n",
    "\n",
    "#inp_data[:,4:8] = Normalizer().fit_transform(f_dataset['X'][:,4:8])\n",
    "#out_data = Normalizer().fit_transform(f_dataset['Y'])\n",
    "\n",
    "inp_data, out_data = normalize(inp_data, out_data)\n",
    "\n",
    "print(np.mean(inp_data, axis =0))\n",
    "print(np.mean(out_data, axis =0))\n",
    "\n",
    "print('Max :', np.max(inp_data, axis =0))\n",
    "print('Max :',np.max(out_data, axis =0))\n",
    "\n",
    "print(inp_data[:,8])\n",
    "\n",
    "print(np.max(inp_data[:,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amber-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset , labelset):\n",
    "        super().__init__()\n",
    "        self.data = torch.from_numpy(dataset)\n",
    "        self.labels = torch.from_numpy(labelset)\n",
    "        \n",
    "               \n",
    "    def __len__(self):\n",
    "        return self.data.size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        out_data = self.labels[idx]\n",
    "        return inp_data, out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "negative-dimension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350000\n"
     ]
    }
   ],
   "source": [
    "full_dataset = SeqDataset(np.array(inp_data , dtype= 'float32'), np.array(out_data, dtype = 'float32') )\n",
    "print(len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sufficient-tulsa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944999 135000 270001\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset.data))\n",
    "test_size = len(full_dataset.data)-train_size -val_size\n",
    "batch = 256\n",
    "print(train_size, val_size, test_size)\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size,val_size, test_size])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader   = data.DataLoader(val_dataset, batch_size=batch)\n",
    "test_loader  = data.DataLoader(test_dataset, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "associate-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=9, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=6, bias=True)\n",
      "  (fc4): Linear(in_features=6, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "    \n",
    "print(model)\n",
    "\n",
    "## Initializing optimizer\n",
    "lr=5e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "verified-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(data, labels, model, device=torch.device(\"cuda:0\")):\n",
    "    target = model.forward(data)\n",
    "    loss = nnf.l1_loss(target, labels)\n",
    "    return loss, target                  # target = predictedc_bolt_tensile\n",
    "\n",
    "\n",
    "def training(model, train_loader, val_loader, epochs, optimizer, checkpoint_path, filename,\n",
    "             device=torch.device(\"cuda:0\"), aux_loss=False):\n",
    "    # Early stopping\n",
    "    patience = 10\n",
    "    trigger_times = 0\n",
    "    min_val_loss = np.inf\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "            # Calculate Train Loss\n",
    "            loss, _ = calculate_loss(data, labels, model)\n",
    "            # back-propagate Train loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), -100)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        for data, labels in val_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "            # Calculate Validation Loss\n",
    "            loss, _ = calculate_loss(data, labels, model)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch + 1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t'\n",
    "            f' Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "        if min_val_loss > val_loss:\n",
    "            print('trigger times: 0')\n",
    "            trigger_times = 0\n",
    "            print(f'Validation Loss Decreased({min_val_loss:.6f}--->{val_loss:.6f}) \\t Saving The Model')\n",
    "            min_val_loss = val_loss\n",
    "            # Saving State Dict\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "            pretrained_filename = os.path.join(checkpoint_path, filename)\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss}, pretrained_filename)\n",
    "\n",
    "        #else:\n",
    "        # trigger_times += 1\n",
    "         #   print('trigger times:', trigger_times)\n",
    "         #   if trigger_times >= patience:\n",
    "         #       print('Early stopping!\\n')\n",
    "          #      return model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-portfolio",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 1/100 [00:25<42:44, 25.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: nan \t\t Validation Loss: nan\n"
     ]
    }
   ],
   "source": [
    "available_filename = ['simple_nn_model.pth']\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"models/trained_models/\"\n",
    "model = training(model, train_loader, val_loader, 100, optimizer, CHECKPOINT_PATH, available_filename[0],device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-salem",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyt] *",
   "language": "python",
   "name": "conda-env-pyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
