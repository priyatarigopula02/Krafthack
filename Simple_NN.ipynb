{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bright-confidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "from collections import defaultdict\n",
    "from optim.optimizer import CosineWarmupScheduler\n",
    "from run.train import training\n",
    "from run.test import test\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.cm import coolwarm\n",
    "plt.set_cmap('cividis')\n",
    "\n",
    "\n",
    "from scipy import spatial\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import itertools\n",
    "from itertools import tee\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "welsh-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9 , 100)  \n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.d = nn.Dropout(p=0.5)\n",
    "        self.fc4 = nn.Linear(100, 6)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #Applying relu non-linearity\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x, negative_slope =0.1)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x, negative_slope =0.1) \n",
    "        x = self.d(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faced-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inp_data, out_data):\n",
    "    #t_start\n",
    "    inp_data[:,0] /= 600    # 10 mins\n",
    "    #t_operating\n",
    "    #inp_data[:,1] /= (86400*5)    # 1 day\n",
    "    inp_data[:,1] = np.log(inp_data[:,1]+1)/20\n",
    "    #t_tracktime\n",
    "    #inp_data[:,2] /= 2592000   # 1 month\n",
    "    inp_data[:,2] = np.log(inp_data[:,2]+1)/20\n",
    "    #t_cumsum\n",
    "    #inp_data[:,2] /= 2592000   # 1 month\n",
    "    inp_data[:,3] = np.log(inp_data[:,3]+1)/30\n",
    "    # power \n",
    "    inp_data[:,4] /= 500\n",
    "    #guidevane\n",
    "    inp_data[:,5] /= 180\n",
    "    # draft_pressure\n",
    "    inp_data[:,6] /= 500\n",
    "    # spiral_pressure\n",
    "    inp_data[:,7] /= 10000\n",
    "    # rotational speed\n",
    "    inp_data[:,8] /= 500\n",
    "    \n",
    "    ## output\n",
    "    out_data /= 2000\n",
    "    \n",
    "    return inp_data, out_data\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alpha-absolute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1350000, 10) (1350000, 6)\n",
      "[1.49223927e+02 1.16750037e+05 8.51013559e+05 9.64145371e+10\n",
      " 2.98626508e+02 1.55861493e+02 5.31218745e+03 1.07874637e+02\n",
      " 9.97639259e-01]\n",
      "[1608.72025976 1487.72319511 1688.32415064 1602.36688526 1636.97079258\n",
      " 1678.00840345]\n",
      "0 0\n",
      "[2.48706546e-01 5.49544591e-01 6.62822448e-01 8.27220053e-01\n",
      " 5.97253016e-01 8.65897185e-01 1.06243749e+01 1.07874637e-02\n",
      " 1.99527852e-03]\n",
      "[0.80436013 0.7438616  0.84416208 0.80118344 0.8184854  0.8390042 ]\n",
      "Max : [4.08333333e-01 6.44532536e-01 7.21967553e-01 8.59446855e-01\n",
      " 6.46604664e-01 1.49086662e+00 1.10255430e+01 1.08368752e-02\n",
      " 2.00000000e-03]\n",
      "Max : [0.81010815 0.74734008 0.84716078 0.80274691 0.81949943 0.84172924]\n",
      "[0.002 0.002 0.002 ... 0.002 0.002 0.002]\n",
      "0.4083333333333333\n"
     ]
    }
   ],
   "source": [
    "f_dataset = np.load('Input_to_NN_notNAN.npz', allow_pickle = True)\n",
    "print(f_dataset['X'].shape, f_dataset['Y'].shape )\n",
    "\n",
    "#inp_data = f_dataset['X'][:, [0,1,2,4,6,7,8,9,10] ]\n",
    "inp_data = f_dataset['X'][:, [0,1,2,3,4,6,7,8,9] ]\n",
    "out_data = f_dataset['Y'] \n",
    "\n",
    "print(np.mean(inp_data, axis =0))\n",
    "print(np.mean(out_data, axis =0))\n",
    "\n",
    "print(np.count_nonzero(np.isnan(f_dataset['X'])), np.count_nonzero(np.isnan(f_dataset['Y'])))\n",
    "\n",
    "#inp_data[:,4:8] = Normalizer().fit_transform(f_dataset['X'][:,4:8])\n",
    "#out_data = Normalizer().fit_transform(f_dataset['Y'])\n",
    "\n",
    "inp_data, out_data = normalize(inp_data, out_data)\n",
    "\n",
    "print(np.mean(inp_data, axis =0))\n",
    "print(np.mean(out_data, axis =0))\n",
    "\n",
    "print('Max :', np.max(inp_data, axis =0))\n",
    "print('Max :',np.max(out_data, axis =0))\n",
    "\n",
    "print(inp_data[:,8])\n",
    "\n",
    "print(np.max(inp_data[:,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sporting-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset , labelset):\n",
    "        super().__init__()\n",
    "        self.data = torch.from_numpy(dataset)\n",
    "        self.labels = torch.from_numpy(labelset)\n",
    "        \n",
    "               \n",
    "    def __len__(self):\n",
    "        return self.data.size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        out_data = self.labels[idx]\n",
    "        return inp_data, out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "answering-domestic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350000\n"
     ]
    }
   ],
   "source": [
    "full_dataset = SeqDataset(np.array(inp_data , dtype= 'float32'), np.array(out_data, dtype = 'float32') )\n",
    "print(len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "danish-assist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944999 135000 270001\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset.data))\n",
    "test_size = len(full_dataset.data)-train_size -val_size\n",
    "batch = 256\n",
    "print(train_size, val_size, test_size)\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size,val_size, test_size])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader   = data.DataLoader(val_dataset, batch_size=batch)\n",
    "test_loader  = data.DataLoader(test_dataset, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "framed-aquatic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=9, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (d): Dropout(p=0.5, inplace=False)\n",
      "  (fc4): Linear(in_features=100, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "    \n",
    "print(model)\n",
    "\n",
    "## Initializing optimizer\n",
    "lr=1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "worldwide-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(data, labels, model, device=torch.device(\"cuda:0\")):\n",
    "    target = model.forward(data)\n",
    "    loss = nnf.l1_loss(target, labels)\n",
    "    l1_lambda = 0.01\n",
    "    l1_reg = torch.tensor(0.).to(device)\n",
    "    for name, params in model.named_parameters() :\n",
    "        if 'fc2' in name:\n",
    "            l1_reg += torch.abs(params).sum()\n",
    "    loss += l1_lambda * l1_reg\n",
    "    return loss, target                  # target = predictedc_bolt_tensile\n",
    "\n",
    "\n",
    "def training(model, train_loader, val_loader, epochs, optimizer, checkpoint_path, filename,\n",
    "             device=torch.device(\"cuda:0\"), aux_loss=False):\n",
    "    min_val_loss = np.inf\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "            # Calculate Train Loss\n",
    "            loss, _ = calculate_loss(data, labels, model)\n",
    "            # back-propagate Train loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        for data, labels in val_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "            # Calculate Validation Loss\n",
    "            loss, _ = calculate_loss(data, labels, model)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch + 1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t'\n",
    "            f' Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "        if min_val_loss > val_loss:\n",
    "            print('trigger times: 0')\n",
    "            trigger_times = 0\n",
    "            print(f'Validation Loss Decreased({min_val_loss:.6f}--->{val_loss:.6f}) \\t Saving The Model')\n",
    "            min_val_loss = val_loss\n",
    "            # Saving State Dict\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "            pretrained_filename = os.path.join(checkpoint_path, filename)\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss}, pretrained_filename)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-orleans",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/trained_models/simple_nn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 1/100 [00:23<38:29, 23.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 0.1017941765346398 \t\t Validation Loss: 0.012372908140109344\n",
      "trigger times: 0\n",
      "Validation Loss Decreased(inf--->6.532895) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▋                                                                                | 2/100 [00:45<36:55, 22.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 \t\t Training Loss: 0.01239572674721334 \t\t Validation Loss: 0.012387494759450696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▍                                                                               | 3/100 [01:07<36:12, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 \t\t Training Loss: 0.012438291685811648 \t\t Validation Loss: 0.012426349921786988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▎                                                                              | 4/100 [01:31<36:45, 22.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 \t\t Training Loss: 0.012571737174761228 \t\t Validation Loss: 0.01242812653424952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|████                                                                              | 5/100 [01:53<35:55, 22.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 \t\t Training Loss: 0.012606720815545674 \t\t Validation Loss: 0.012484482113822278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▉                                                                             | 6/100 [02:16<35:30, 22.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 \t\t Training Loss: 0.012494735475008855 \t\t Validation Loss: 0.012595368778530621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▋                                                                            | 7/100 [02:38<34:55, 22.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 \t\t Training Loss: 0.012405325748698558 \t\t Validation Loss: 0.01258484332356602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▌                                                                           | 8/100 [03:02<35:13, 22.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 \t\t Training Loss: 0.012357765124907374 \t\t Validation Loss: 0.012147835101297294\n",
      "trigger times: 0\n",
      "Validation Loss Decreased(6.532895--->6.414057) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████▍                                                                          | 9/100 [03:25<34:41, 22.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 \t\t Training Loss: 0.012305180795755176 \t\t Validation Loss: 0.012053076998034323\n",
      "trigger times: 0\n",
      "Validation Loss Decreased(6.414057--->6.364025) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████                                                                         | 10/100 [03:47<34:08, 22.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 \t\t Training Loss: 0.012272729863372559 \t\t Validation Loss: 0.012251704133990588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▉                                                                        | 11/100 [04:09<33:32, 22.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 \t\t Training Loss: 0.012248762902170137 \t\t Validation Loss: 0.012037898376795718\n",
      "trigger times: 0\n",
      "Validation Loss Decreased(6.364025--->6.356010) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▋                                                                       | 12/100 [04:32<32:58, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 \t\t Training Loss: 0.01222019345860144 \t\t Validation Loss: 0.012051523006944493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|██████████▌                                                                      | 13/100 [04:54<32:29, 22.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 \t\t Training Loss: 0.012197607193839595 \t\t Validation Loss: 0.012117979338985275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████▎                                                                     | 14/100 [05:17<32:20, 22.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 \t\t Training Loss: 0.01218362750353827 \t\t Validation Loss: 0.012474738841057953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▏                                                                    | 15/100 [05:41<32:37, 23.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 \t\t Training Loss: 0.012177822548483274 \t\t Validation Loss: 0.011879787020117157\n",
      "trigger times: 0\n",
      "Validation Loss Decreased(6.356010--->6.272528) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|████████████▉                                                                    | 16/100 [06:04<32:27, 23.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 \t\t Training Loss: 0.012151771670636835 \t\t Validation Loss: 0.011846602139430064\n",
      "trigger times: 0\n",
      "Validation Loss Decreased(6.272528--->6.255006) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▊                                                                   | 17/100 [06:27<31:52, 23.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 \t\t Training Loss: 0.012142756104265422 \t\t Validation Loss: 0.012191662614262013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▌                                                                  | 18/100 [06:51<31:47, 23.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 \t\t Training Loss: 0.012136308098349653 \t\t Validation Loss: 0.012446105330265269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████▍                                                                 | 19/100 [07:17<32:28, 24.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 \t\t Training Loss: 0.012122644321446463 \t\t Validation Loss: 0.012088634632880601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▏                                                                | 20/100 [07:40<31:40, 23.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 \t\t Training Loss: 0.01211024922263498 \t\t Validation Loss: 0.011987733458888462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "available_filename = ['simple_nn_model.pth']\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"models/trained_models/\"\n",
    "\n",
    "\n",
    "# Check whether pretrained model exists. If yes, load it and skip training\n",
    "pretrained_filename = os.path.join(CHECKPOINT_PATH, available_filename[0]  )\n",
    "print(pretrained_filename)\n",
    "if os.path.isfile(pretrained_filename):\n",
    "    print(\"Found pretrained model, loading...\")\n",
    "    checkpoint = torch.load(pretrained_filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    val_loss = checkpoint['val_loss']\n",
    "    print(epoch)\n",
    "else:\n",
    "    model = training(model, train_loader, val_loader, 100, optimizer, CHECKPOINT_PATH, available_filename[0],device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-labor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyt] *",
   "language": "python",
   "name": "conda-env-pyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
